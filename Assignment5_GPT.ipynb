{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Build and Train a GPT Model\n",
    "\n",
    "**PSYC 51.17: Models of Language and Communication**\n",
    "\n",
    "**Due Date: February 13, 2026 at 11:59 PM EST**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this capstone-style assignment, you will build a GPT (Generative Pre-trained Transformer) model from the ground up and train it to generate text.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Implement transformer architecture components (attention, FFN, layer norm)\n",
    "- Understand autoregressive language modeling\n",
    "- Implement causal (masked) self-attention\n",
    "- Build a complete training pipeline\n",
    "- Apply different text generation strategies\n",
    "- Analyze learned representations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Installation](#1-setup-and-installation)\n",
    "2. [Dataset Selection and Loading](#2-dataset-selection-and-loading)\n",
    "3. [Transformer Components](#3-transformer-components)\n",
    "4. [GPT Model](#4-gpt-model)\n",
    "5. [Training Pipeline](#5-training-pipeline)\n",
    "6. [Text Generation](#6-text-generation)\n",
    "7. [Analysis and Visualization](#7-analysis-and-visualization)\n",
    "8. [Comparison with Pre-trained Models](#8-comparison-with-pre-trained-models)\n",
    "9. [Conclusion](#9-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers\n",
    "!pip install -q datasets tiktoken\n",
    "!pip install -q matplotlib seaborn plotly\n",
    "!pip install -q tqdm numpy pandas\n",
    "!pip install -q wandb  # Optional: for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Selection and Loading\n",
    "\n",
    "Choose a dataset:\n",
    "- Shakespeare (small, good for testing)\n",
    "- Code (Python from GitHub)\n",
    "- Stories/Creative Writing\n",
    "- Domain-specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Shakespeare dataset (small, good for initial testing)\n",
    "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
    "\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nSample text:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - you can use character-level or BPE\n",
    "# Option A: Character-level (simpler)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Option B: Use GPT-2 tokenizer (more realistic)\n",
    "# from transformers import GPT2Tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Train/val split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Val: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Components\n",
    "\n",
    "Implement the core building blocks of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256  # context length\n",
    "n_embd = 384      # embedding dimension\n",
    "n_head = 6        # number of attention heads\n",
    "n_layer = 6       # number of transformer blocks\n",
    "dropout = 0.2\n",
    "learning_rate = 3e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Multi-Head Self-Attention with Causal Masking\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention with causal masking.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout, block_size):\n",
    "        super().__init__()\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with attention and FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout, block_size):\n",
    "        super().__init__()\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT Model\n",
    "\n",
    "Assemble the complete GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement GPT Model\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
    "        super().__init__()\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"Generate text autoregressively.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_layer=n_layer,\n",
    "    block_size=block_size,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Print model size\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Your training loop here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Generation\n",
    "\n",
    "Implement different sampling strategies: greedy, temperature, top-k, nucleus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate text with different strategies\n",
    "prompt = \"ROMEO:\"\n",
    "context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "# Greedy decoding\n",
    "print(\"Greedy decoding:\")\n",
    "# ...\n",
    "\n",
    "# Temperature sampling\n",
    "print(\"\\nTemperature sampling (T=0.8):\")\n",
    "# ...\n",
    "\n",
    "# Top-k sampling\n",
    "print(\"\\nTop-k sampling (k=40):\")\n",
    "# ...\n",
    "\n",
    "# Nucleus sampling\n",
    "print(\"\\nNucleus sampling (p=0.9):\")\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize attention patterns\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize token embeddings (UMAP or t-SNE)\n",
    "# Your implementation here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare with GPT-2\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Your comparison here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "*Summarize your implementation, discuss training dynamics, analyze results, and reflect on what you learned.*\n",
    "\n",
    "TODO: Your conclusion here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
